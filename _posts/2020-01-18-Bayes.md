---
layout:     post
title:      Bayes
subtitle:     贝叶斯
date:       2020-01-18
author:     Young
header-img: img/1*oKi6F9CNeCyhLajj_RRSoA.png
catalog: true
tags:
    - machine learning
    - python
---



### [什么是极大似然估计](https://blog.csdn.net/zengxiantao1994/article/details/72787849)

<div class="htmledit_views" id="content_views">
  <h1><a name="t1"></a><a name="t1"></a><span style="font-size:18px;">贝叶斯决策</span></h1>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 首先来看贝叶斯分类，我们都知道经典的贝叶斯公式：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528002022807" alt=""><br></span></p>
  <p align="center"></p><p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 其中：<span style="font-size:18px;">p(w)</span>：为先验概率，表示每种类别分布的概率；<img src="https://img-blog.csdn.net/20170528002108539" alt="">：类条件概率，表示在某种类别前提下，某事发生的概率；而<img src="https://img-blog.csdn.net/20170528002145196" alt="">为后验概率，表示某事发生了，并且它属于某一类别的概率，有了这个后验概率，我们就可以对样本进行分类。后验概率越大，说明某事物属于这个类别的可能性越大，我们越有理由把它归到这个类别下。</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 我们来看一个直观的例子：<strong>已知：</strong>在夏季，某公园男性穿凉鞋的概率为1/2，女性穿凉鞋的概率为2/3，并且该公园中男女比例通常为2:1，<strong>问题：</strong>若你在公园中随机遇到一个穿凉鞋的人，请问他的性别为男性或女性的概率分别为多少？</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 从问题看，就是上面讲的，某事发生了，它属于某一类别的概率是多少？即后验概率。</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 设：<img src="https://img-blog.csdn.net/20170528002248527" alt=""></span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 由已知可得：</span></p>
  <p style="text-align:center;"><img src="https://img-blog.csdn.net/20170528002344387" alt=""></p>
  <p><span style="font-size:18px;"></span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 男性和女性穿凉鞋相互独立，所以<img src="https://img-blog.csdn.net/20170528002436496" alt=""></span></p>
  <p><span style="font-size:18px;">（若只考虑分类问题，只需要比较后验概率的大小，的取值并不重要）。</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 由贝叶斯公式算出：<img src="https://img-blog.csdn.net/20170528002504950" alt=""></span></p>
  <p align="center"></p>
  <p align="center"></p>
  <p align="center"><br></p>
  <h1><a name="t2"></a><a name="t2"></a><span style="font-size:18px;">问题引出</span></h1>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 但是在实际问题中并不都是这样幸运的，我们能获得的数据可能只有有限数目的样本数据，而先验概率<img src="https://img-blog.csdn.net/20170528002627998" alt="">和类条件概率(各类的总体分布)<img src="https://img-blog.csdn.net/20170528002633154" alt="">都是未知的。根据仅有的样本数据进行分类时，一种可行的办法是我们需要先对先验概率和类条件概率进行估计，然后再套用贝叶斯分类器。</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 先验概率的估计较简单，1、每个样本所属的自然状态都是已知的（有监督学习）；2、依靠经验；3、用训练样本中各类出现的频率估计。</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 类条件概率的估计（非常难），原因包括：概率密度函数包含了一个随机变量的全部信息；样本数据可能不多；特征向量x的维度可能很大等等。总之要直接估计类条件概率的密度函数很难。解决的办法就是，把估计完全未知的概率密度<img src="https://img-blog.csdn.net/20170528002633154" alt="">转化为估计参数。<span style="color:#FF0000;">这里就将概率密度估计问题转化为参数估计问题，极大似然估计就是一种参数估计方法。</span>当然了，概率密度函数的选取很重要，模型正确，在样本区域无穷时，我们会得到较准确的估计值，如果模型都错了，那估计半天的参数，肯定也没啥意义了。</span></p>
  <p><span style="font-size:18px;"><br></span></p>
  <h1><a name="t3"></a><a name="t3"></a><span style="font-size:18px;">重要前提</span></h1>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 上面说到，参数估计问题只是实际问题求解过程中的一种简化方法（由于直接估计类条件概率密度函数很困难）。所以能够使用极大似然估计方法的样本必须需要满足一些前提假设。</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span style="color:#FF0000;">重要前提：训练样本的分布能代表样本的真实分布。每个样本集中的样本都是所谓独立同分布的随机变量</span><span style="color:#FF0000;"> (iid</span><span style="color:#FF0000;">条件</span><span style="color:#FF0000;">)</span><span style="color:#FF0000;">，且有充分的训练样本</span><span style="color:#FF0000;">。</span></span></p>
  <p><span style="font-size:18px;"><span style="color:#FF0000;"><br></span></span></p>
  <h1><a name="t4"></a><a name="t4"></a><span style="font-size:18px;">极大似然估计</span></h1>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 极大似然估计的原理，用一张图片来说明，如下图所示：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528002827749" alt=""><br></span></p>
  <p align="center"></p><p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 总结起来，最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 由于样本集中的样本都是独立同分布，可以只考虑一类样本集D，来估计参数向量θ。记已知的样本集为：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528003138251" alt=""><br></span></p>
  <p align="center"></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 似然函数（linkehood function）：联合概率密度函数<img src="https://img-blog.csdn.net/20170528003212360" alt="">称为相对于<img src="https://img-blog.csdn.net/20170528003218392" alt="">的θ的似然函数。</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528003223845" alt=""><br></span></p>
  <p align="center"></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 如果<img src="https://img-blog.csdn.net/20170528003231366" alt="">是参数空间中能使似然函数<img src="https://img-blog.csdn.net/20170528003236220" alt="">最大的θ值，则<img src="https://img-blog.csdn.net/20170528003231366" alt="">应该是“最可能”的参数值，那么<img src="https://img-blog.csdn.net/20170528003231366" alt="">就是θ的极大似然估计量。它是样本集的函数，记作：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528003244189" alt=""><br></span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><br></span></p>
  <h1><a name="t5"></a><a name="t5"></a><span style="font-size:18px;">求解极大似然函数</span></h1>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; ML估计：求使得出现该组样本的概率最大的θ值。</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528003838359" alt=""><br></span></p><p align="center"></p>
  <p><span style="font-size:18px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 实际中为了便于分析，定义了对数似然函数：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528003844453" alt=""><br></span></p>
  <p align="center"><img src="https://img-blog.csdn.net/2018060522232071" alt=""></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 1. 未知参数只有一个（θ为标量）</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 在似然函数满足连续、可微的正则条件下，极大似然估计量是下面微分方程的解：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528003855734" alt=""><br></span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 2.未知参数有多个（θ为向量）</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 则θ可表示为具有S个分量的未知向量：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528003901066" alt=""><br></span></p>
  <p align="center"></p><p><span style="font-size:18px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 记梯度算子：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528003905766" alt=""><br></span></p>
  <p align="center"></p><p><span style="font-size:18px;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 若似然函数满足连续可导的条件，则最大似然估计量就是如下方程的解。</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528003911078" alt=""><br></span></p>
  <p align="center"></p>
  <p><span style="font-size:18px;"><span style="color:#FF0000;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span style="color:#FF0000;">方程的解只是一个估计值，只有在样本数趋于无限多的时候，它才会接近于真实值。</span></span></p>
  <p><span style="font-size:18px;"><span style="color:#FF0000;"><br></span></span></p>
  <h1><a name="t6"></a><a name="t6"></a><span style="font-size:18px;">极大似然估计的例子</span></h1>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 例1：设样本服从正态分布<img src="https://img-blog.csdn.net/20170528003917176" alt="" style="font-size:18px;">，则似然函数为：</span></p><p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528003922141" alt=""><br></span></p>
  <p align="center"></p><p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 它的对数：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528003926973" alt=""><br></span></p>
  <p align="center"></p><p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 求导，得方程组：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528004731774" alt=""><br></span></p>
  <p></p>
  <p align="center"></p><p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 联合解得：</span></p>
  <p style="text-align:center;"><img src="https://img-blog.csdn.net/20170528004738060" alt=""><br></p>
  <p align="center"></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 似然方程有唯一解<img src="https://img-blog.csdn.net/20170528004743185" alt="" style="font-size:18px;text-align:center;">：，而且它一定是最大值点，这是因为当<img src="https://img-blog.csdn.net/20170528004747290" alt="" style="font-size:18px;text-align:center;">或<img src="https://img-blog.csdn.net/20170528004751982" alt="" style="font-size:18px;text-align:center;">时，非负函数<img src="https://img-blog.csdn.net/20170528004756951" alt="" style="font-size:18px;text-align:center;">。于是U</span><span style="font-size:18px;">和<img src="https://img-blog.csdn.net/20170528004801165" alt="" style="font-size:18px;text-align:center;">的极大似然估计为<img src="https://img-blog.csdn.net/20170528004743185" alt="" style="font-size:18px;text-align:center;">。</span></p><p><span style="font-size:18px;"><br></span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 例2：设样本服从均匀分布[a, b]。则X的概率密度函数：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528005253964" alt=""><br></span></p>
  <p align="center"></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 对样本<img src="https://img-blog.csdn.net/20170528005300323" alt="">：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528005304097" alt=""><br></span></p>
  <p align="center"></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 很显然，L(a,b)作为a和b的二元函数是不连续的，这时不能用导数来求解。而必须从极大似然估计的定义出发，求L(a,b)的最大值，为使L(a,b)达到最大，b-a应该尽可能地小，但b又不能小于<img src="https://img-blog.csdn.net/20170528005307589" alt="">，否则，L(a,b)=0。类似地a不能大过<img src="https://img-blog.csdn.net/20170528005311058" alt="">，因此，a和b的极大似然估计：</span></p>
  <p style="text-align:center;"><span style="font-size:18px;"><img src="https://img-blog.csdn.net/20170528005314613" alt=""><br></span></p>
  <p align="left"></p>
  <p align="left"><br></p>
  <p align="center"></p>
  <h1><a name="t7"></a><a name="t7"></a><span style="font-size:18px;">总结</span></h1>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 求最大似然估计量<img src="https://img-blog.csdn.net/20170528003231366" alt="">的一般步骤：</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; （1）写出似然函数；</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; （2）对似然函数取对数，并整理；</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; （3）求导数；</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; （4）解似然方程。</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 最大似然估计的特点：</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 1.比其他估计方法更加简单；</span>
</p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 2.收敛性：无偏或者渐近无偏，当样本数目增加时，收敛性质会更好；</span></p>
  <p><span style="font-size:18px;">&nbsp; &nbsp; &nbsp; &nbsp; 3.如果假设的类条件概率模型正确，则通常能获得较好的结果。但如果假设模型出现偏差，将导致非常差的估计结果。</span></p>
  <p><span style="font-size:18px;"><br></span></p>
</div>









### 基础概念

- 先验概率、条件概率、后验概率
  
  - 条件概率**: 就是事件A在事件B发生的条件下发生的概率。
    - **条件概率，就是在条件为瓜的颜色是青绿的情况下，瓜是好瓜的概率**
  
  - 先验概率**: 在贝叶斯统计中，某一不确定量 p 的先验概率分布是在考虑"观测数据"前，能表达 p 不确定性的概率分布。它旨在描述这个不确定量的不确定程度，而不是这个不确定量的随机性。这个不确定量可以是一个参数，或者是一个隐含变量。
    - **先验概率，就是常识、经验、统计学所透露出的“因”的概率，即瓜的颜色是青绿的概率。**
  
  - **后验概率**: 在贝叶斯统计中，一个随机事件或者一个不确定事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。同样，后验概率分布是一个未知量（视为随机变量）基于试验和调查后得到的概率分布。“后验”在本文中代表考虑了被测试事件的相关证据。
    - **后验概率，就是在知道“果”之后，去推测“因”的概率，也就是说，如果已经知道瓜是好瓜，那么瓜的颜色是青绿的概率是多少**。后验和先验的关系就需要运用贝叶斯决策理论来求解。
  
- **联合概率分布**

  - Probability of two (or more) simultaneous events

  - 朴素贝叶斯法是典型的生成学习方法。**生成方法由训练数据学习联合概率分布** $P(X,Y)$，**然后求得后验概率分布** $P(Y|X)$ 。具体来说，利用训练数据学习 $P(X|Y)$ 和 $P(Y)$ 的估计，得到联合概率分布：

    $$
    P(X,Y)＝P(Y)P(X|Y)
    $$

- **条件概率分布**
  
  - Probability of one (or more) event given the occurrence of another event
  
  $$
  P\left(X=x | Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right), \quad k=1,2, \cdots, K
  $$
  
  - 条件概率分布 $P(X=x \mid Y=c_k)$ 有指数级的数量的参数，假设 $x^{(j)}$ 可取值有 $S_j$ 个，$j=1,2,...,n$，Y可取值有 K 个，那么参数个数为 $K\prod_{j=1}^{n}S_j$。因为 X 作为特征向量维度往往很大，任意特征组合在一起的概率都考虑的话几乎没法统计出来。
  
- **条件独立性假设**
  
  - 假设条件相互独立，求解的参数大大减少，只需要单独求解每一个 $p(x^i \mid y)$ 然后相乘**。
  
    $$
\begin{aligned} P\left(X=x | Y=c_{k}\right) &=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right) \\ &=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right) \end{aligned}
    $$
  
  - **朴素贝叶斯法利用贝叶斯定理与学到的联合概率模型进行分类预测**，对给定的输入 x，通过学习到的模型计算后验概率分布$P(Y=c_k \mid X=x)$，将后验概率最大的类作为 x 的类输出。后验概率计算根据贝叶斯定理进行：
  
    $$
    P\left(Y=c_{k} | X=x\right)=\frac{P(X,Y)}{P(X)}=\frac{P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}
    $$
    
    $$
    \begin{array}{c} \\ {P\left(Y=c_{k} | X=x\right)=\frac{{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)},}\end{array}, \quad k=1,2, \cdots, K
    $$
    
  
  <br>
    故**朴素贝叶斯分类器**可表示为：
  
  $$
    \begin{array}{c} \\ {y=f(x)=\arg \max _{c_{k}} \frac{{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}}\end{array}
  $$

  $$
  y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)
  $$

- **后验概率最大化**的含义





